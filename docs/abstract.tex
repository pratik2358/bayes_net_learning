\begin{abstract}
Bayesian Network (BN) structure learning plays a crucial role in representing probabilistic dependencies among variables in complex datasets. This study investigates the impact of data quantity on the performance of BN structure learning using different algorithms. We evaluate the exact structure learning algorithm, both with and without a maximum parent constraint, alongside the Chow-Liu algorithm, to compare the resulting network structures. Structural accuracy is assessed by matching the parent sets of the nodes between the learned and the true structures. Additionally, we present Kullback-Leibler (KL) divergences between the joint, conditional, and probability distributions derived from the data and the learned models, conditioned on the inferred structures. Our experiments offer insights into how the amount of data affects both structural accuracy and the fidelity of the learned networks, providing guidance for optimizing the structure learning process in practical applications.
\end{abstract}